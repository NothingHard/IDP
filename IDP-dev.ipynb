{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guarantee_initialized_variables(session, list_of_variables = None):\n",
    "    if list_of_variables is None:\n",
    "        list_of_variables = tf.global_variables()\n",
    "    uninitialized_variables = list(tf.get_variable(name) for name in\n",
    "                                   session.run(tf.report_uninitialized_variables(list_of_variables)))\n",
    "    # session.run(tf.initialize_variables(uninitialized_variables))\n",
    "    return unintialized_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self,profile,params=None):\n",
    "        self.x = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.y = tf.placeholder(tf.int32,[None,10])\n",
    "        self.loss_counter = 0\n",
    "        self.profile = profile\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        if params==None:\n",
    "            self.W1 = tf.get_variable(initializer=tf.random_uniform([784, 100],-0.1,0.1),name=\"W1\")\n",
    "            self.b1 = tf.get_variable(initializer=tf.zeros([100])+0.1,name=\"b1\")\n",
    "            self.W2 = tf.get_variable(initializer=tf.random_uniform([100, 100],-0.1,0.1),name=\"W2\")\n",
    "            self.b2 = tf.get_variable(initializer=tf.zeros([100])+0.1,name=\"b2\")\n",
    "            self.W3 = tf.get_variable(initializer=tf.random_uniform([100, 10],-0.1,0.1),name=\"W3\")\n",
    "            self.b3 = tf.get_variable(initializer=tf.zeros([10])+0.1,name=\"b3\")\n",
    "            self.idp = np.arange(0.1,1.05,0.1)\n",
    "        else:\n",
    "            self.W1 = tf.get_variable(initializer=params['W1'],name=\"W1\")\n",
    "            self.b1 = tf.get_variable(initializer=params['b1'],name=\"b1\")\n",
    "            self.W2 = tf.get_variable(initializer=params['W2'],name=\"W2\")\n",
    "            self.b2 = tf.get_variable(initializer=params['b2'],name=\"b2\")\n",
    "            self.W3 = tf.get_variable(initializer=params['W3'],name=\"W3\")\n",
    "            self.b3 = tf.get_variable(initializer=params['b3'],name=\"b3\")\n",
    "            self.idp = params['idp']\n",
    "            \n",
    "        def half_exp(n,k=1,dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - int(n/2)\n",
    "            return np.append(np.ones(n_ones,dtype=dtype),np.exp((1-k)*np.arange(n_other),dtype=dtype))\n",
    "\n",
    "        if profile == \"linear\":\n",
    "            self.r1 = tf.get_variable(initializer=np.linspace(1,0,num=784,endpoint=False,dtype='float32'),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=np.linspace(1,0,num=100,endpoint=False,dtype='float32'),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=np.linspace(1,0,num=100,endpoint=False,dtype='float32') ,name=\"r3\",dtype='float32')\n",
    "        elif profile == \"all-one\":\n",
    "            self.r1 = tf.get_variable(initializer=np.ones(784,dtype='float32'),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=np.ones(100,dtype='float32'),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=np.ones(100,dtype='float32'),name=\"r3\",dtype='float32')\n",
    "        elif profile == \"half-exp\":\n",
    "            self.r1 = tf.get_variable(initializer=half_exp(784,2),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=half_exp(100,2),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=half_exp(100,2),name=\"r3\",dtype='float32')\n",
    "        else:\n",
    "            self.r1 = tf.get_variable(initializer=np.array(1.0/(np.arange(784)+1),dtype='float32'),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=np.array(1.0/(np.arange(100)+1),dtype='float32'),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=np.array(1.0/(np.arange(100)+1),dtype='float32') ,name=\"r3\",dtype='float32')\n",
    "        \n",
    "        ''' define a mask '''\n",
    "        for idp in self.idp:\n",
    "            with tf.variable_scope(self.profile+str(idp),reuse=None):\n",
    "                n_ones = int(100 * idp)\n",
    "                n_zeros = 100 - n_ones\n",
    "                t2 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t2\",dtype='float32',trainable=False)\n",
    "                # p2 = tf.get_variable(initializer = tf.multiply(self.r2,t2),name=\"p2\",dtype='float32')\n",
    "                print(\"ones=%d, zeros=%d\" % (n_ones,n_zeros))\n",
    "        \n",
    "        # calculate r1*(W1*x)+b\n",
    "        z1 = tf.add(tf.matmul(self.x,self.W1),self.b1)\n",
    "        y1 = tf.nn.relu(z1)\n",
    "        with tf.variable_scope(self.profile+str(self.idp[self.loss_counter]),reuse=True):\n",
    "            t2 = tf.get_variable(\"t2\")\n",
    "            p2 = tf.multiply(self.r2,t2)\n",
    "            z2 = tf.add(tf.matmul(tf.multiply(p2,y1),self.W2),self.b2)\n",
    "        y2 = tf.nn.relu(z2)\n",
    "        logits = tf.add(tf.matmul(y2,self.W3),self.b3)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.y)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(probs,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        tvars_trainable = tf.trainable_variables()\n",
    "        print(tvars_trainable)\n",
    "        self.gamma_vars = [self.r1,self.r2,self.r3]\n",
    "\n",
    "        for rm in self.gamma_vars:\n",
    "            tvars_trainable.remove(rm)\n",
    "            print('%s is not trainable.'% rm)\n",
    "        self.tvars_trainable = tvars_trainable\n",
    "\n",
    "        '''non-increasing weight clip'''\n",
    "        def clip_in_order(last_output,current_input):\n",
    "            added = tf.cond(current_input > last_output[0], lambda: last_output[0], lambda: current_input)\n",
    "            return (added,last_output[1])\n",
    "        clipped_gamma = tf.scan(fn=clip_in_order,\n",
    "                                elems = self.r2,\n",
    "                                initializer = (self.r2[0],self.r2[1]))\n",
    "        self.gamma_clip_manually = tf.assign(ref=self.r2,value=clipped_gamma[0])\n",
    "\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,var_list=self.tvars_trainable)\n",
    "        \n",
    "        self.train_op_gamma = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,var_list=self.gamma_vars)\n",
    "    \n",
    "    def set_trained_idp(self, sess, idp):\n",
    "        ''' calculate crossentropy at different idp level and sum up as the loss '''\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        for tidp in idp:\n",
    "            z1 = tf.add(tf.matmul(self.x,self.W1),self.b1)\n",
    "            y1 = tf.nn.relu(z1)\n",
    "            with tf.variable_scope(self.profile+str(tidp),reuse=True):\n",
    "                t2 = tf.get_variable(\"t2\")\n",
    "                p2 = tf.multiply(self.r2,t2)\n",
    "                z2 = tf.add(tf.matmul(tf.multiply(p2,y1),self.W2),self.b2)\n",
    "            y2 = tf.nn.relu(z2)\n",
    "            logits = tf.add(tf.matmul(y2,self.W3),self.b3)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.y)\n",
    "            this_loss = tf.reduce_mean(cross_entropy) # * self.loss_mask[mask_counter]\n",
    "            loss = loss + this_loss\n",
    "#             loss = tf.reduce_mean(cross_entropy)\n",
    "#             loss_list.append(loss)\n",
    "#         self.loss = tf.reduce_mean(loss)   \n",
    "#         self.loss = tf.reduce_mean(loss_list[self.loss_counter])\n",
    "#         self.loss = loss_list[self.loss_counter]\n",
    "        self.loss = loss\n",
    "    \n",
    "    def set_optimizer(self, sess, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            optim1 = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            optim2 = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        \n",
    "            self.train_op = optim1.minimize(self.loss,var_list=self.tvars_trainable)\n",
    "            self.train_op_gamma = optim2.minimize(self.loss,var_list=self.gamma_vars)\n",
    "        \n",
    "        tvars_trainable = tf.trainable_variables()\n",
    "        print(tvars_trainable)\n",
    "        for rm in self.gamma_vars:\n",
    "            tvars_trainable.remove(rm)\n",
    "            print('%s is not trainable.'% rm)\n",
    "        self.tvars_trainable = tvars_trainable\n",
    "        \n",
    "        #init_vars = tf.variables_initializer(\n",
    "        #    [v for v in tf.global_variables() if v.name.split(':')[0] in set(sess.run(tf.report_uninitialized_variables()))\n",
    "        #])\n",
    "        \n",
    "        for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "            sess.run(v.initializer)\n",
    "        print(\"Initialized.\")\n",
    "        #sess.run(init_vars)\n",
    "#         self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,var_list=self.tvars_trainable)\n",
    "#         self.train_op_gamma = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,var_list=self.gamma_vars)\n",
    "\n",
    "        \n",
    "        #sess.run(tf.initialize_variables([optim1.get_slot(self.loss, name) for name in optim1.get_slot_names()]))\n",
    "        #sess.run(tf.initialize_variables([optim2.get_slot(self.loss, name) for name in optim2.get_slot_names()]))\n",
    "#         uninitialized = guarantee_initialized_variables(sess)\n",
    "\n",
    "#         for v in uninitiazlized:\n",
    "#             sess.run(v.initializer)\n",
    "#         print(\"Initialized.\")\n",
    "\n",
    "        \n",
    "    def train(self,sess,config,gamma_trainable=False,reuse=False,verbose=True):\n",
    "        \n",
    "        self.learning_rate = config['learning_rate']\n",
    "\n",
    "        \"\"\" setting from config \"\"\"\n",
    "        epochs = config['epochs']\n",
    "        batch_per_epoch = config['batch_per_epoch']\n",
    "        batch_size = config['batch_size']\n",
    "        save_dir = config['save_dir']\n",
    "        mnist = config['mnist']\n",
    "        # loss_mask = config['loss_mask']\n",
    "\n",
    "        \"\"\" record container \"\"\"\n",
    "        val_loss_per_epoch = []\n",
    "        val_accu_per_epoch = []\n",
    "        loss_per_epoch = []\n",
    "        accu_per_epoch = []\n",
    "        gamma_trainable_per_epoch = []\n",
    "        \n",
    "        if not reuse:\n",
    "            for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "                sess.run(v.initializer)\n",
    "            print(\"Initialized.\")\n",
    "\n",
    "        \"\"\" training \"\"\"\n",
    "        for epoch in range(0,epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss = []\n",
    "            train_accu = []\n",
    "            for batch in range(0,batch_per_epoch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                if gamma_trainable:\n",
    "                    _,loss, accu = sess.run([self.train_op_gamma, self.loss, self.accuracy],\n",
    "                                            feed_dict = {self.x:batch_x, self.y: batch_y})\n",
    "                                            # feed_dict = {self.x:batch_x, self.y: batch_y, self.loss_mask: loss_mask})\n",
    "                    _ = sess.run([self.gamma_clip_manually])\n",
    "\n",
    "                else:\n",
    "                    _,loss, accu = sess.run([self.train_op, self.loss, self.accuracy], \n",
    "                                            feed_dict = {self.x:batch_x, self.y: batch_y})\n",
    "                                            # feed_dict = {self.x:batch_x, self.y: batch_y, self.loss_mask: loss_mask})\n",
    "                train_loss.append(loss)\n",
    "                train_accu.append(accu)\n",
    "\n",
    "            loss_per_epoch.append(np.mean(train_loss))\n",
    "            accu_per_epoch.append(np.mean(train_accu))\n",
    "            gamma_trainable_per_epoch.append(gamma_trainable+0)\n",
    "\n",
    "            \"\"\" validation \"\"\"\n",
    "            val_loss , val_accu = sess.run([self.loss,self.accuracy],\n",
    "                                            feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels})\n",
    "                                            # feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels, self.loss_mask: loss_mask})\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "            val_accu_per_epoch.append(val_accu)\n",
    "\n",
    "            if verbose:\n",
    "                print(\" epoch: %3d \\t time: %3f \\t loss: %3f \\t val_loss: %3f \\t accu: %3f \\t val_accu: %3f\"% \n",
    "                  (epoch,time.time() - start_time,np.mean(train_loss), val_loss, np.mean(train_accu), val_accu))\n",
    "\n",
    "        return({'loss':loss_per_epoch, 'val_loss':val_loss_per_epoch,\n",
    "                'accu':accu_per_epoch,'val_accu':val_accu_per_epoch,\n",
    "                'gamma_trainable':gamma_trainable_per_epoch})\n",
    "    \n",
    "    def predict(self,sess,config,idp,scope,reuse=None):\n",
    "        with tf.variable_scope(scope,reuse=reuse):\n",
    "            # use variable t to control the on/off of neurons\n",
    "            n_ones = int(784 * idp)\n",
    "            n_zeros = 784 - n_ones\n",
    "            t1 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t1\",dtype='float32')\n",
    "\n",
    "            n_ones = int(100 * idp)\n",
    "            n_zeros = 100 - n_ones\n",
    "            t2 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t2\",dtype='float32')\n",
    "\n",
    "            n_ones = int(100 * idp)\n",
    "            n_zeros = 100 - n_ones\n",
    "            t3 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t3\",dtype='float32')\n",
    "\n",
    "#             p1 = tf.get_variable(initializer=tf.multiply(self.r1,t1),name=\"p1\")\n",
    "#             p2 = tf.get_variable(initializer=tf.multiply(self.r2,t2),name=\"p2\")\n",
    "#             p3 = tf.get_variable(initializer=tf.multiply(self.r3,t3),name=\"p3\")\n",
    "            p1 = tf.multiply(self.r1,t1)\n",
    "            p2 = tf.multiply(self.r2,t2)\n",
    "            p3 = tf.multiply(self.r3,t3)\n",
    "            \n",
    "            for v in [t1,t2,t3]:\n",
    "                sess.run(v.initializer)  \n",
    "            print(\"Running IDP mechanism {:.0f}%\".format(idp*100),end='\\r')\n",
    "            # print(sess.run(p2))\n",
    "            z1 = tf.add(tf.matmul(self.x,self.W1),self.b1)\n",
    "            y1 = tf.nn.relu(z1)\n",
    "            # IDP on layer 2\n",
    "            z2 = tf.add(tf.matmul(tf.multiply(p2,y1),self.W2),self.b2)\n",
    "            y2 = tf.nn.relu(z2)\n",
    "            logits = tf.add(tf.matmul(y2,self.W3),self.b3)\n",
    "            self.pred_probs = tf.nn.softmax(logits)\n",
    "\n",
    "            mnist = config['mnist']\n",
    "            pred_probs = sess.run([self.pred_probs],\n",
    "                                  feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels})\n",
    "                                  # feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels, self.loss_mask: loss_mask})\n",
    "            return(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "profile_test = [\"all-one\",\"linear\",\"harmonic\",\"half-exp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(high=0.1,low=-0.1,size=(784,100)).astype('float32')\n",
    "b1 = np.random.uniform(high=0.1,low=-0.1,size=(100)).astype('float32')\n",
    "W2 = np.random.uniform(high=0.1,low=-0.1,size=(100,100)).astype('float32')\n",
    "b2 = np.random.uniform(high=0.1,low=-0.1,size=(100)).astype('float32')   \n",
    "W3 = np.random.uniform(high=0.1,low=-0.1,size=(100,10)).astype('float32')\n",
    "b3 = np.random.uniform(high=0.1,low=-0.1,size=(10)).astype('float32')\n",
    "params = {'W1':W1,'W2':W2,'W3':W3,\n",
    "          'b1':b1,'b2':b2,'b3':b3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "profile_test = [\"all-one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## training using gradually complete loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ ITERATIVE TRAINING ============\n",
      "Start 1 round\n",
      "ones=10, zeros=90\n",
      "ones=20, zeros=80\n",
      "ones=30, zeros=70\n",
      "ones=40, zeros=60\n",
      "ones=50, zeros=50\n",
      "ones=60, zeros=40\n",
      "ones=70, zeros=30\n",
      "ones=80, zeros=20\n",
      "ones=90, zeros=10\n",
      "ones=100, zeros=0\n",
      "[<tf.Variable 'all-one1/W1:0' shape=(784, 100) dtype=float32_ref>, <tf.Variable 'all-one1/b1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/W2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'all-one1/b2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/W3:0' shape=(100, 10) dtype=float32_ref>, <tf.Variable 'all-one1/b3:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'all-one1/r1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/r2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/r3:0' shape=(100,) dtype=float32_ref>]\n",
      "<tf.Variable 'all-one1/r1:0' shape=(784,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'all-one1/r2:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'all-one1/r3:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "Start to optimize IDP = 10 \n",
      "Initialized.\n",
      " epoch:   0 \t time: 0.736868 \t loss: 1.349871 \t val_loss: 0.682596 \t accu: 0.573929 \t val_accu: 0.766200\n",
      "IDP=0.10, accuracy=0.766%\n",
      "IDP=0.20, accuracy=0.765%\n",
      "IDP=0.30, accuracy=0.758%\n",
      "IDP=0.40, accuracy=0.756%\n",
      "IDP=0.50, accuracy=0.757%\n",
      "IDP=0.60, accuracy=0.760%\n",
      "IDP=0.70, accuracy=0.760%\n",
      "IDP=0.80, accuracy=0.761%\n",
      "IDP=0.90, accuracy=0.762%\n",
      "IDP=1.00, accuracy=0.7580%\n",
      "[ 0.96070445  0.96070445  0.96070445  0.96030182  0.96030182  0.96030182\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674  0.91679674\n",
      "  0.91679674  0.91679674  0.91679674  0.91679674]\n",
      "IDP=0.10, accuracy=0.766%\n",
      "IDP=0.20, accuracy=0.766%\n",
      "IDP=0.30, accuracy=0.760%\n",
      "IDP=0.40, accuracy=0.759%\n",
      "IDP=0.50, accuracy=0.759%\n",
      "IDP=0.60, accuracy=0.761%\n",
      "IDP=0.70, accuracy=0.763%\n",
      "IDP=0.80, accuracy=0.761%\n",
      "IDP=0.90, accuracy=0.762%\n",
      "IDP=1.00, accuracy=0.7580%\n",
      "current focused performance: 0.766200\n",
      "============ ITERATIVE TRAINING ============\n",
      "Start 2 round\n",
      " epoch:   0 \t time: 0.549544 \t loss: 0.594225 \t val_loss: 0.478950 \t accu: 0.816786 \t val_accu: 0.860200\n",
      "IDP=0.10, accuracy=0.860%\n",
      "IDP=0.20, accuracy=0.860%\n",
      "IDP=0.30, accuracy=0.858%\n",
      "IDP=0.40, accuracy=0.857%\n",
      "IDP=0.50, accuracy=0.857%\n",
      "IDP=0.60, accuracy=0.858%\n",
      "IDP=0.70, accuracy=0.857%\n",
      "IDP=0.80, accuracy=0.855%\n",
      "IDP=0.90, accuracy=0.855%\n",
      "IDP=1.00, accuracy=0.8550%\n",
      "[ 0.91868168  0.91868168  0.91688198  0.91176832  0.91176832  0.90730023\n",
      "  0.90081584  0.90081584  0.90081584  0.90081584  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443  0.89143443\n",
      "  0.89143443  0.89143443  0.89143443  0.89143443]\n",
      "IDP=0.10, accuracy=0.860%\n",
      "IDP=0.20, accuracy=0.860%\n",
      "IDP=0.30, accuracy=0.859%\n",
      "IDP=0.40, accuracy=0.859%\n",
      "IDP=0.50, accuracy=0.858%\n",
      "IDP=0.60, accuracy=0.859%\n",
      "IDP=0.70, accuracy=0.858%\n",
      "IDP=0.80, accuracy=0.858%\n",
      "IDP=0.90, accuracy=0.858%\n",
      "IDP=1.00, accuracy=0.8560%\n",
      "current focused performance: 0.860400\n",
      "[<tf.Variable 'all-one1/W1:0' shape=(784, 100) dtype=float32_ref>, <tf.Variable 'all-one1/b1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/W2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'all-one1/b2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/W3:0' shape=(100, 10) dtype=float32_ref>, <tf.Variable 'all-one1/b3:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'all-one1/r1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/r2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/r3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i10/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i10/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i10/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i20/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i20/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i20/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i30/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i30/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i30/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i40/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i40/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i40/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i50/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i50/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i50/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i60/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i60/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i60/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i70/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i70/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i70/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i80/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i80/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i80/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i90/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i90/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i90/t3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i100/t1:0' shape=(784,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i100/t2:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'all-one1/all-one1i100/t3:0' shape=(100,) dtype=float32_ref>]\n",
      "<tf.Variable 'all-one1/r1:0' shape=(784,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'all-one1/r2:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'all-one1/r3:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "Initialized.\n",
      "Start to optimize IDP = 20 \n",
      "============ ITERATIVE TRAINING ============\n",
      "Start 3 round\n",
      " epoch:   0 \t time: 0.563213 \t loss: 1.141847 \t val_loss: 0.508389 \t accu: 0.375357 \t val_accu: 0.420300\n",
      "IDP=0.10, accuracy=0.420%\n",
      "IDP=0.20, accuracy=0.853%\n",
      "IDP=0.30, accuracy=0.850%\n",
      "IDP=0.40, accuracy=0.851%\n",
      "IDP=0.50, accuracy=0.851%\n",
      "IDP=0.60, accuracy=0.852%\n",
      "IDP=0.70, accuracy=0.848%\n",
      "IDP=0.80, accuracy=0.848%\n",
      "IDP=0.90, accuracy=0.848%\n",
      "IDP=1.00, accuracy=0.8470%\n",
      "[ 0.96929467  0.96929467  0.96696222  0.96301103  0.96202552  0.95817459\n",
      "  0.92658061  0.92658061  0.92658061  0.92658061  0.92658061  0.92658061\n",
      "  0.92658061  0.92658061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061  0.89033061\n",
      "  0.89033061  0.89033061  0.89033061  0.89033061]\n",
      "IDP=0.10, accuracy=0.423%\n",
      "IDP=0.20, accuracy=0.852%\n",
      "IDP=0.30, accuracy=0.849%\n",
      "IDP=0.40, accuracy=0.849%\n",
      "IDP=0.50, accuracy=0.849%\n",
      "IDP=0.60, accuracy=0.852%\n",
      "IDP=0.70, accuracy=0.846%\n",
      "IDP=0.80, accuracy=0.845%\n",
      "IDP=0.90, accuracy=0.845%\n",
      "IDP=1.00, accuracy=0.8430%\n",
      "current focused performance: 0.851700\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable all-one1/all-one1/all-one1/W1/Adam/ already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-c98d854053b9>\", line 124, in set_optimizer\n    self.train_op = optim1.minimize(self.loss,var_list=self.tvars_trainable)\n  File \"<ipython-input-10-e162d11412fd>\", line 111, in <module>\n    model_test.set_optimizer(sess=sess, scope = model_test.profile+str(counter))\n  File \"/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e162d11412fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trained_idp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtidp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtidp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c98d854053b9>\u001b[0m in \u001b[0;36mset_optimizer\u001b[0;34m(self, sess, scope)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtvars_trainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op_gamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 325\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    444\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_get_variable_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    172\u001b[0m     return create_slot_with_initializer(\n\u001b[1;32m    173\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[1;32m    175\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 146\u001b[0;31m                                 dtype)\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_is_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    662\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 664\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable all-one1/all-one1/all-one1/W1/Adam/ already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-c98d854053b9>\", line 124, in set_optimizer\n    self.train_op = optim1.minimize(self.loss,var_list=self.tvars_trainable)\n  File \"<ipython-input-10-e162d11412fd>\", line 111, in <module>\n    model_test.set_optimizer(sess=sess, scope = model_test.profile+str(counter))\n  File \"/Users/chunmingchang/tensorflow/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "all_epochs = 100\n",
    "counter = counter + 1\n",
    "profile = \"all-one\"\n",
    "# loss_mask = np.zeros(10)\n",
    "params['idp'] = np.arange(0.1,1.05,0.1)\n",
    "# params['idp'] = [0.1]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope(profile+str(counter)) as scope:\n",
    "        intv = 0\n",
    "        tidp = 0\n",
    "        var_reuse = False\n",
    "        go_on = True\n",
    "        current_best_accu = 0\n",
    "        current_best_counter = 0\n",
    "        \"\"\" result containers \"\"\"\n",
    "        profile_arr = []\n",
    "        idp_arr = []\n",
    "        accu_arr = []\n",
    "        r2_dict = {}\n",
    "        log = pd.DataFrame()\n",
    "        while go_on:\n",
    "            # loss_mask[tidp] = 1\n",
    "            save_dir = \"~/IDP/output/\"\n",
    "            epochs = 1\n",
    "            batch_per_epoch = 200\n",
    "            batch_size = 28\n",
    "            learning_rate = 0.001\n",
    "            config = {'epochs': epochs,\n",
    "                      'batch_per_epoch': batch_per_epoch,\n",
    "                      'batch_size': batch_size,\n",
    "                      'learning_rate': learning_rate,\n",
    "                      'save_dir': save_dir,\n",
    "                      'log_file': save_dir+profile+\"_\"+str(tidp*100)+\"_log.csv\",\n",
    "                      'result_file': save_dir+profile+\"_\"+str(tidp*100)+\"_result.csv\",\n",
    "                      'r2_file':save_dir+profile+\"_\"+str(tidp*100)+\"_r2.csv\",\n",
    "                      'mnist': mnist,\n",
    "                     }\n",
    "                      # 'loss_mask':loss_mask}\n",
    "            \n",
    "            print(\"============ ITERATIVE TRAINING ============\")\n",
    "            #for _ in range(int(all_epochs/epochs/2)):  \n",
    "            intv = intv + 1\n",
    "            print(\"Start \"+str(intv)+\" round\")\n",
    "\n",
    "            ''' training W,b '''\n",
    "            if intv == 1:\n",
    "                model_test = model(profile,params=params)\n",
    "                model_test.loss_counter = tidp\n",
    "                print(\"Start to optimize IDP = %d \" % ((tidp+1)*10))\n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=True)\n",
    "                log = pd.DataFrame.from_dict(log1)\n",
    "                ''' record initial gamma of layer 2 '''\n",
    "                r2_dict['initial'] = sess.run(model_test.r2)\n",
    "            else:\n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=True)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "                log = pd.concat([log,log1])\n",
    "            \n",
    "            \n",
    "            ''' test after W,b updates '''\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            for idp in np.arange(0.1,1.05,0.1):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)),reuse=var_reuse)\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                profile_arr.append(profile+\"(train W,b in the \"+str(intv)+\" epochs)\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "            var_reuse = True\n",
    "\n",
    "            ''' training gamma '''\n",
    "            #scope.reuse_variables()\n",
    "\n",
    "            log1 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=var_reuse,verbose=False)\n",
    "            log1 = pd.DataFrame.from_dict(log1)\n",
    "            log = pd.concat([log,log1])\n",
    "            r2_dict['after_'+str(intv)] = sess.run(model_test.r2)\n",
    "            \n",
    "            print(r2_dict['after_'+str(intv)])\n",
    "\n",
    "            ''' test after gamma updates '''\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            this_accu = []\n",
    "            for idp in np.arange(0.1,1.05,0.1):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)),reuse=var_reuse)\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                profile_arr.append(profile+\"(train gamma in the \"+str(intv)+\" epochs)\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "                # accu list\n",
    "                this_accu.append(accu)\n",
    "            \n",
    "            if np.sum(this_accu) > current_best_accu:\n",
    "                current_best_accu = np.sum(this_accu)\n",
    "                current_best_counter = 0\n",
    "            else:\n",
    "                current_best_counter = current_best_counter + 1\n",
    "            \n",
    "            print(\"current focused performance: %f\" % (this_accu[tidp]))\n",
    "            \n",
    "            # if converged and wait for 3 epochs, next idp point\n",
    "            if (current_best_counter >= 5) or (this_accu[tidp] >= np.min([0.8+tidp*0.03,0.999])):\n",
    "                tidp = tidp + 1\n",
    "                current_best_counter = 0\n",
    "                \n",
    "                model_test.set_trained_idp(sess=sess, idp=[params['idp'][tidp]])\n",
    "                model_test.set_optimizer(sess=sess, scope = model_test.profile+str(counter))\n",
    "                \n",
    "                model_test.loss_counter = tidp\n",
    "                print(\"Start to optimize IDP = %d \" % ((tidp+1)*10))\n",
    "            \n",
    "            # go over the whole tidp point\n",
    "            if tidp == 10:\n",
    "                go_on = False\n",
    "                                      \n",
    "        r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "        r2.to_csv(config['r2_file'],index=None)\n",
    "        log.to_csv(config['log_file'],index=None)\n",
    "        res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "        res.to_csv(config['result_file'],index=None)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "r2.to_csv(config['r2_file'],index=None)\n",
    "log.to_csv(config['log_file'],index=None)\n",
    "res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "res.to_csv(config['result_file'],index=None)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(profile+str(1)) as scope:\n",
    "    with tf.variable_scope(profile+str(0.1)) as scope2:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        #z2 = tf.add(tf.matmul(tf.multiply(p2,y1),self.W2),self.b2)\n",
    "            sess.run(model_test.p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alternate training using different loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_idp = [0.05]\n",
    "all_epochs = 20\n",
    "counter = counter + 1\n",
    "profile = \"all-one\"\n",
    "for tidp in target_idp:\n",
    "    params['idp'] = np.array([tidp])\n",
    "    interval = 1\n",
    "    save_dir = \"~/IDP/output/\"\n",
    "    epochs = interval\n",
    "    batch_per_epoch = 200\n",
    "    batch_size = 28\n",
    "    learning_rate = 0.001\n",
    "    config = {'epochs': epochs,\n",
    "              'batch_per_epoch': batch_per_epoch,\n",
    "              'batch_size': batch_size,\n",
    "              'learning_rate': learning_rate,\n",
    "              'save_dir': save_dir,\n",
    "              'log_file': save_dir+profile+\"_\"+str(tidp*100)+\"_log.csv\",\n",
    "              'result_file': save_dir+profile+\"_\"+str(tidp*100)+\"_result.csv\",\n",
    "              'r2_file':save_dir+profile+\"_\"+str(tidp*100)+\"_r2.csv\",\n",
    "              'mnist': mnist}\n",
    "    print(profile)\n",
    "    \n",
    "    '''result containers'''\n",
    "    profile_arr = []\n",
    "    idp_arr = []\n",
    "    accu_arr = []\n",
    "    r2_dict = {}\n",
    "    log = pd.DataFrame()\n",
    "    with tf.Session() as sess:\n",
    "        with tf.variable_scope(str(tidp)+str(counter)+str(epochs)) as scope:            \n",
    "            print(\"============ ITERATIVE TRAINING ============\")\n",
    "            var_reuse = False\n",
    "            for intv in range(int(all_epochs/interval/2)):  \n",
    "                print(\"Start \"+str(intv)+\" round\")\n",
    "                '''training W,b'''\n",
    "                if intv == 0:\n",
    "                    model_test = model(profile,params=params)\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log = pd.DataFrame.from_dict(log1)\n",
    "                    ''' record initial gamma of layer 2'''\n",
    "                    r2_dict['initial'] = sess.run(model_test.r2)\n",
    "                    var_reuse = True\n",
    "                else:\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log1 = pd.DataFrame.from_dict(log1)\n",
    "                    log = pd.concat([log,log1])\n",
    "                \n",
    "                '''test after W,b updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train W,b in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                \n",
    "                '''training gamma'''\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=var_reuse,verbose=False)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "                log = pd.concat([log,log1])\n",
    "                r2_dict['after_'+str(intv)] = sess.run(model_test.r2)\n",
    "                \n",
    "                '''test after gamma updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train gamma in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                                  \n",
    "            r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "            r2.to_csv(config['r2_file'],index=None)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            res.to_csv(config['result_file'],index=None)\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alternate training between gamma and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_epochs = 20\n",
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    save_dir = \"~/IDP/output/\"\n",
    "    epochs = 1\n",
    "    batch_per_epoch = 200\n",
    "    batch_size = 28\n",
    "    learning_rate = 0.001\n",
    "    config = {'epochs': epochs,\n",
    "              'batch_per_epoch': batch_per_epoch,\n",
    "              'batch_size': batch_size,\n",
    "              'learning_rate': learning_rate,\n",
    "              'save_dir': save_dir,\n",
    "              'log_file': save_dir+profile+\"_\"+str(epochs)+\"_log.csv\",\n",
    "              'result_file': save_dir+profile+\"_\"+str(epochs)+\"_result.csv\",\n",
    "              'r2_file':save_dir+profile+\"_\"+str(epochs)+\"_r2.csv\",\n",
    "              'mnist': mnist}\n",
    "    print(profile)\n",
    "    \n",
    "    '''result containers'''\n",
    "    profile_arr = []\n",
    "    idp_arr = []\n",
    "    accu_arr = []\n",
    "    r2_dict = {}\n",
    "    log = pd.DataFrame()\n",
    "    with tf.Session() as sess:\n",
    "        with tf.variable_scope(profile+str(counter)+str(epochs)) as scope:            \n",
    "            print(\"============ ITERATIVE TRAINING ============\")\n",
    "            var_reuse = False\n",
    "            for intv in range(int(all_epochs/epoch/2)):  \n",
    "                print(\"Start \"+str(intv)+\" round\")\n",
    "                '''training W,b'''\n",
    "                if intv == 0:\n",
    "                    model_test = model(profile,params=params)\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log = pd.DataFrame.from_dict(log1)\n",
    "                    ''' record initial gamma of layer 2'''\n",
    "                    r2_dict['initial'] = sess.run(model_test.r2)\n",
    "                    var_reuse = True\n",
    "                else:\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log1 = pd.DataFrame.from_dict(log1)\n",
    "                    log = pd.concat([log,log1])\n",
    "                \n",
    "                '''test after W,b updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train W,b in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                \n",
    "                '''training gamma'''\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=var_reuse,verbose=False)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "                log = pd.concat([log,log1])\n",
    "                r2_dict['after_'+str(intv)] = sess.run(model_test.r2)\n",
    "                \n",
    "                '''test after gamma updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train gamma in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                                  \n",
    "            r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "            r2.to_csv(config['r2_file'],index=None)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            res.to_csv(config['result_file'],index=None)\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Study on when we start training gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs_test = np.array([5])\n",
    "profile_test = [\"linear\",\"half-exp\",\"all-one\",\"harmonic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_epochs = 100\n",
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    for epochs in epochs_test:\n",
    "        save_dir = \"~/IDP/output/\"\n",
    "        # epochs = 20\n",
    "        batch_per_epoch = 200\n",
    "        batch_size = 28\n",
    "        learning_rate = 0.001\n",
    "        config = {'epochs': epochs,\n",
    "                  'batch_per_epoch': batch_per_epoch,\n",
    "                  'batch_size': batch_size,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'save_dir': save_dir,\n",
    "                  'log_file': save_dir+profile+\"_\"+str(epochs)+\"_log.csv\",\n",
    "                  'result_file': save_dir+profile+\"_\"+str(epochs)+\"_result.csv\",\n",
    "                  'r2_file':save_dir+profile+\"_\"+str(epochs)+\"_r2.csv\",\n",
    "                  'mnist': mnist}\n",
    "        print(profile)\n",
    "        '''result containers'''\n",
    "        profile_arr = []\n",
    "        idp_arr = []\n",
    "        accu_arr = []\n",
    "        r2_dict = {}\n",
    "        with tf.Session() as sess:\n",
    "            with tf.variable_scope(profile+str(counter)+str(epochs)) as scope:\n",
    "                model_test = model(profile,params=params)\n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+str(epochs)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    profile_arr.append(profile+\"(train W,b in the first \"+str(config['epochs'])+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                    \n",
    "                ''' record current gamma of layer 2'''\n",
    "                r2_dict['before'] = sess.run(model_test.r2)\n",
    "                \n",
    "                print(\"============ FURTHER TRAINING ============\")\n",
    "                scope.reuse_variables()\n",
    "                config['epochs'] = all_epochs - config['epochs']\n",
    "                log2 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=True,verbose=False)\n",
    "                log2 = pd.DataFrame.from_dict(log2)\n",
    "                log = pd.concat([log1,log2])\n",
    "                log.to_csv(config['log_file'],index=None)\n",
    "                #print(log)\n",
    "\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+str(epochs)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    profile_arr.append(profile+\"(train gamma in the last \"+str(config['epochs'])+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                    \n",
    "                ''' record current gamma of layer 2'''\n",
    "                r2_dict['after'] = sess.run(model_test.r2)\n",
    "                \n",
    "                r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "                r2.to_csv(config['r2_file'],index=None)\n",
    "                \n",
    "                res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "                res.to_csv(config['result_file'],index=None)\n",
    "                print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Half-half training experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    save_dir = \"~/IDP/output/\"\n",
    "    epochs = 1\n",
    "    batch_per_epoch = 200\n",
    "    batch_size = 28\n",
    "    learning_rate = 0.001\n",
    "    config = {'epochs': epochs,\n",
    "              'batch_per_epoch': batch_per_epoch,\n",
    "              'batch_size': batch_size,\n",
    "              'learning_rate': learning_rate,\n",
    "              'save_dir': save_dir,\n",
    "              'log_file': save_dir+profile+\"_log.csv\",\n",
    "              'result_file': save_dir+profile+\"_result.csv\",\n",
    "              'mnist': mnist}\n",
    "    print(profile)\n",
    "    '''result containers'''\n",
    "    profile_arr = []\n",
    "    idp_arr = []\n",
    "    accu_arr = []\n",
    "    with tf.Session() as sess:\n",
    "        with tf.variable_scope(profile+str(counter)) as scope:\n",
    "            model_test = model(profile,params=params)\n",
    "            log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "            log1 = pd.DataFrame.from_dict(log1)\n",
    "\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                profile_arr.append(profile+\"(fixed gamma ep\"+str(config['epochs'])+\")\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "\n",
    "            print(\"============ FURTHER TRAINING ============\")\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            log2 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=True,verbose=False)\n",
    "            log2 = pd.DataFrame.from_dict(log2)\n",
    "            log = pd.concat([log1,log2])\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            #print(log)\n",
    "\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                profile_arr.append(profile+\"(trainable gamma ep\"+str(config['epochs'])+\")\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "            #res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            #res.to_csv(config['result_file'],index=None)\n",
    "            #print(res)\n",
    "            \n",
    "        print(\"============ TRAINING AS USUAL ============\")\n",
    "        \n",
    "        with tf.variable_scope(profile+str(counter)+\"normal\"):\n",
    "            model_test = model(profile=profile,params=params)\n",
    "            save_dir = \"~/IDP/output/\"\n",
    "            epochs = 80\n",
    "            batch_per_epoch = 200\n",
    "            batch_size = 28\n",
    "            learning_rate = 0.001\n",
    "            config = {'epochs': epochs,\n",
    "                      'batch_per_epoch': batch_per_epoch,\n",
    "                      'batch_size': batch_size,\n",
    "                      'learning_rate': learning_rate,\n",
    "                      'save_dir': save_dir,\n",
    "                      'log_file': save_dir+profile+\"_log2.csv\",\n",
    "                      'result_file': save_dir+profile+\"_result.csv\",\n",
    "                      'mnist': mnist}\n",
    "            with tf.Session() as sess:\n",
    "                log = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "                log = pd.DataFrame.from_dict(log)\n",
    "                log.to_csv(config['log_file'],index=None)\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    profile_arr.append(profile+\"(fixed gamma ep\"+str(config['epochs'])+\")\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                log = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "                log.to_csv(config['result_file'],index=None)\n",
    "                print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    print(profile)\n",
    "    with tf.variable_scope(profile+str(counter)):\n",
    "        model_test = model(profile=profile,params=params)\n",
    "        save_dir = \"~/IDP/output/new_loss_\"\n",
    "        epochs = 20\n",
    "        batch_per_epoch = 200\n",
    "        batch_size = 28\n",
    "        learning_rate = 0.002\n",
    "        config = {'epochs': epochs,\n",
    "                  'batch_per_epoch': batch_per_epoch,\n",
    "                  'batch_size': batch_size,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'save_dir': save_dir,\n",
    "                  'log_file': save_dir+profile+\"_log.csv\",\n",
    "                  'result_file': save_dir+profile+\"_result.csv\",\n",
    "                  'mnist': mnist}\n",
    "        with tf.Session() as sess:\n",
    "            log = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "            log = pd.DataFrame.from_dict(log)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            print(log)\n",
    "            \n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            profile_arr = []\n",
    "            idp_arr = []\n",
    "            accu_arr = []\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                profile_arr.append(profile+\"(fixed gamma ep\"+str(config['epochs'])+\")\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "            log = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            log.to_csv(config['result_file'],index=None)\n",
    "            print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
