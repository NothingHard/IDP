{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self,profile,params=None):\n",
    "        self.x = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.y = tf.placeholder(tf.int32,[None,10])\n",
    "        self.loss_counter = 0\n",
    "        self.profile = profile\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        if params==None:\n",
    "            self.W1 = tf.get_variable(initializer=tf.random_uniform([784, 100],-0.1,0.1),name=\"W1\")\n",
    "            self.b1 = tf.get_variable(initializer=tf.zeros([100])+0.1,name=\"b1\")\n",
    "            self.W2 = tf.get_variable(initializer=tf.random_uniform([100, 100],-0.1,0.1),name=\"W2\")\n",
    "            self.b2 = tf.get_variable(initializer=tf.zeros([100])+0.1,name=\"b2\")\n",
    "            self.W3 = tf.get_variable(initializer=tf.random_uniform([100, 10],-0.1,0.1),name=\"W3\")\n",
    "            self.b3 = tf.get_variable(initializer=tf.zeros([10])+0.1,name=\"b3\")\n",
    "            self.idp = np.arange(0.1,1.05,0.1)\n",
    "        else:\n",
    "            self.W1 = tf.get_variable(initializer=params['W1'],name=\"W1\")\n",
    "            self.b1 = tf.get_variable(initializer=params['b1'],name=\"b1\")\n",
    "            self.W2 = tf.get_variable(initializer=params['W2'],name=\"W2\")\n",
    "            self.b2 = tf.get_variable(initializer=params['b2'],name=\"b2\")\n",
    "            self.W3 = tf.get_variable(initializer=params['W3'],name=\"W3\")\n",
    "            self.b3 = tf.get_variable(initializer=params['b3'],name=\"b3\")\n",
    "            self.idp = params['idp']\n",
    "            \n",
    "        def half_exp(n,k=1,dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - int(n/2)\n",
    "            return np.append(np.ones(n_ones,dtype=dtype),np.exp((1-k)*np.arange(n_other),dtype=dtype))\n",
    "\n",
    "        if profile == \"linear\":\n",
    "            self.r1 = tf.get_variable(initializer=np.linspace(1,0,num=784,endpoint=False,dtype='float32'),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=np.linspace(1,0,num=100,endpoint=False,dtype='float32'),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=np.linspace(1,0,num=100,endpoint=False,dtype='float32') ,name=\"r3\",dtype='float32')\n",
    "        elif profile == \"all-one\":\n",
    "            self.r1 = tf.get_variable(initializer=np.ones(784,dtype='float32'),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=np.ones(100,dtype='float32'),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=np.ones(100,dtype='float32'),name=\"r3\",dtype='float32')\n",
    "        elif profile == \"half-exp\":\n",
    "            self.r1 = tf.get_variable(initializer=half_exp(784,2),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=half_exp(100,2),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=half_exp(100,2),name=\"r3\",dtype='float32')\n",
    "        else:\n",
    "            self.r1 = tf.get_variable(initializer=np.array(1.0/(np.arange(784)+1),dtype='float32'),name=\"r1\",dtype='float32')\n",
    "            self.r2 = tf.get_variable(initializer=np.array(1.0/(np.arange(100)+1),dtype='float32'),name=\"r2\",dtype='float32')\n",
    "            self.r3 = tf.get_variable(initializer=np.array(1.0/(np.arange(100)+1),dtype='float32') ,name=\"r3\",dtype='float32')\n",
    "        \n",
    "        ''' define a mask '''\n",
    "        for idp in self.idp:\n",
    "            with tf.variable_scope(self.profile+str(idp),reuse=None):\n",
    "                n_ones = int(100 * idp)\n",
    "                n_zeros = 100 - n_ones\n",
    "                t2 = tf.constant(np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t2\",dtype='float32')\n",
    "                p2 = tf.multiply(self.r2,t2)\n",
    "\n",
    "        ''' calculate crossentropy at different idp level and sum up as the loss '''\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        for idp in self.idp:\n",
    "            z1 = tf.add(tf.matmul(self.x,self.W1),self.b1)\n",
    "            y1 = tf.nn.relu(z1)\n",
    "            with tf.variable_scope(self.profile+str(idp),reuse=True):\n",
    "                z2 = tf.add(tf.matmul(tf.multiply(p2,y1),self.W2),self.b2)\n",
    "            y2 = tf.nn.relu(z2)\n",
    "            logits = tf.add(tf.matmul(y2,self.W3),self.b3)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=self.y)\n",
    "            # this_loss = tf.reduce_mean(cross_entropy) * self.loss_mask[mask_counter]\n",
    "            # mask_counter = mask_counter + 1\n",
    "            this_loss = tf.reduce_mean(cross_entropy)\n",
    "            loss = loss + this_loss\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        # calculate r1*(W1*x)+b\n",
    "        z1 = tf.add(tf.matmul(self.x,self.W1),self.b1)\n",
    "        y1 = tf.nn.relu(z1)\n",
    "        z2 = tf.add(tf.matmul(tf.multiply(self.r2,y1),self.W2),self.b2)\n",
    "        y2 = tf.nn.relu(z2)\n",
    "        self.logits = tf.add(tf.matmul(y2,self.W3),self.b3)\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,labels=self.y)\n",
    "        # self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.loss = loss_list[self.loss_counter]\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.probs,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        tvars_trainable = tf.trainable_variables()\n",
    "        # print(tvars_trainable)\n",
    "        self.gamma_vars = [self.r1,self.r2,self.r3]\n",
    "\n",
    "        for rm in self.gamma_vars:\n",
    "            tvars_trainable.remove(rm)\n",
    "            print('%s is not trainable.'% rm)\n",
    "        self.tvars_trainable = tvars_trainable\n",
    "\n",
    "        '''non-increasing weight clip'''\n",
    "        def clip_in_order(last_output,current_input):\n",
    "            added = tf.cond(current_input > last_output[0], lambda: last_output[0], lambda: current_input)\n",
    "            return (added,last_output[1])\n",
    "        clipped_gamma = tf.scan(fn=clip_in_order,\n",
    "                                elems = self.r2,\n",
    "                                initializer = (self.r2[0],self.r2[1]))\n",
    "        self.gamma_clip_manually = tf.assign(ref=self.r2,value=clipped_gamma[0])\n",
    "\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,var_list=self.tvars_trainable)\n",
    "        \n",
    "        self.train_op_gamma = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,var_list=self.gamma_vars)\n",
    "    \n",
    "    def train(self,sess,config,gamma_trainable=False,reuse=False,verbose=True):\n",
    "        \n",
    "        self.learning_rate = config['learning_rate']\n",
    "\n",
    "        \"\"\" setting from config \"\"\"\n",
    "        epochs = config['epochs']\n",
    "        batch_per_epoch = config['batch_per_epoch']\n",
    "        batch_size = config['batch_size']\n",
    "        save_dir = config['save_dir']\n",
    "        mnist = config['mnist']\n",
    "        input_counter = config['loss_counter']\n",
    "        # loss_mask = config['loss_mask']\n",
    "\n",
    "        \"\"\" record container \"\"\"\n",
    "        val_loss_per_epoch = []\n",
    "        val_accu_per_epoch = []\n",
    "        loss_per_epoch = []\n",
    "        accu_per_epoch = []\n",
    "        gamma_trainable_per_epoch = []\n",
    "        \n",
    "        if not reuse:\n",
    "            for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "                sess.run(v.initializer)\n",
    "            print(\"Initialized.\")\n",
    "            \n",
    "        # print(loss_mask)\n",
    "\n",
    "        \"\"\" training \"\"\"\n",
    "        for epoch in range(0,epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss = []\n",
    "            train_accu = []\n",
    "            for batch in range(0,batch_per_epoch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                if gamma_trainable:\n",
    "                    _,loss, accu = sess.run([self.train_op_gamma, self.loss, self.accuracy],\n",
    "                                            feed_dict = {self.x:batch_x, self.y: batch_y})\n",
    "                                            # feed_dict = {self.x:batch_x, self.y: batch_y, self.loss_mask: loss_mask})\n",
    "                    _ = sess.run([self.gamma_clip_manually])\n",
    "\n",
    "                else:\n",
    "                    _,loss, accu = sess.run([self.train_op, self.loss, self.accuracy], \n",
    "                                            feed_dict = {self.x:batch_x, self.y: batch_y})\n",
    "                                            # feed_dict = {self.x:batch_x, self.y: batch_y, self.loss_mask: loss_mask})\n",
    "                train_loss.append(loss)\n",
    "                train_accu.append(accu)\n",
    "            # print(sess.run(self.r2))\n",
    "            loss_per_epoch.append(np.mean(train_loss))\n",
    "            accu_per_epoch.append(np.mean(train_accu))\n",
    "            gamma_trainable_per_epoch.append(gamma_trainable+0)\n",
    "\n",
    "            \"\"\" validation \"\"\"\n",
    "            val_loss , probs, val_accu = sess.run([self.loss,self.probs,self.accuracy],\n",
    "                                                  feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels})\n",
    "                                                  # feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels, self.loss_mask: loss_mask})\n",
    "            val_loss_per_epoch.append(val_loss)\n",
    "            val_accu_per_epoch.append(val_accu)\n",
    "\n",
    "            if verbose:\n",
    "                print(\" epoch: %3d \\t time: %3f \\t loss: %3f \\t val_loss: %3f \\t accu: %3f \\t val_accu: %3f\"% \n",
    "                  (epoch,time.time() - start_time,np.mean(train_loss), val_loss, np.mean(train_accu), val_accu))\n",
    "\n",
    "        return({'loss':loss_per_epoch, 'val_loss':val_loss_per_epoch,\n",
    "                'accu':accu_per_epoch,'val_accu':val_accu_per_epoch,\n",
    "                'gamma_trainable':gamma_trainable_per_epoch})\n",
    "    \n",
    "    def predict(self,sess,config,idp,scope):\n",
    "        with tf.variable_scope(scope,reuse=None):\n",
    "            # use variable t to control the on/off of neurons\n",
    "            n_ones = int(784 * idp)\n",
    "            n_zeros = 784 - n_ones\n",
    "            t1 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t1\",dtype='float32')\n",
    "\n",
    "            n_ones = int(100 * idp)\n",
    "            n_zeros = 100 - n_ones\n",
    "            t2 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t2\",dtype='float32')\n",
    "\n",
    "            n_ones = int(100 * idp)\n",
    "            n_zeros = 100 - n_ones\n",
    "            t3 = tf.get_variable(initializer=np.append(np.ones(n_ones,dtype='float32'),np.zeros(n_zeros,dtype='float32')),name=\"t3\",dtype='float32')\n",
    "\n",
    "            p1 = tf.get_variable(initializer=tf.multiply(self.r1,t1),name=\"p1\")\n",
    "            p2 = tf.get_variable(initializer=tf.multiply(self.r2,t2),name=\"p2\")\n",
    "            p3 = tf.get_variable(initializer=tf.multiply(self.r3,t3),name=\"p3\")\n",
    "\n",
    "            for v in [t1,t2,t3,p1,p2,p3]:\n",
    "                sess.run(v.initializer)\n",
    "            print(\"Running IDP mechanism {:.0f}%\".format(idp*100),end='\\r')\n",
    "\n",
    "            z1 = tf.add(tf.matmul(self.x,self.W1),self.b1)\n",
    "            y1 = tf.nn.relu(z1)\n",
    "            # IDP on layer 2\n",
    "            z2 = tf.add(tf.matmul(tf.multiply(p2,y1),self.W2),self.b2)\n",
    "            y2 = tf.nn.relu(z2)\n",
    "            logits = tf.add(tf.matmul(y2,self.W3),self.b3)\n",
    "            self.pred_probs = tf.nn.softmax(logits)\n",
    "\n",
    "            mnist = config['mnist']\n",
    "            pred_probs = sess.run([self.pred_probs],\n",
    "                                  feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels})\n",
    "                                  # feed_dict={self.x:mnist.test.images, self.y:mnist.test.labels, self.loss_mask: loss_mask})\n",
    "            return(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "profile_test = [\"all-one\",\"linear\",\"harmonic\",\"half-exp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(high=0.1,low=-0.1,size=(784,100)).astype('float32')\n",
    "b1 = np.random.uniform(high=0.1,low=-0.1,size=(100)).astype('float32')\n",
    "W2 = np.random.uniform(high=0.1,low=-0.1,size=(100,100)).astype('float32')\n",
    "b2 = np.random.uniform(high=0.1,low=-0.1,size=(100)).astype('float32')   \n",
    "W3 = np.random.uniform(high=0.1,low=-0.1,size=(100,10)).astype('float32')\n",
    "b3 = np.random.uniform(high=0.1,low=-0.1,size=(10)).astype('float32')\n",
    "params = {'W1':W1,'W2':W2,'W3':W3,\n",
    "          'b1':b1,'b2':b2,'b3':b3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_test = [\"all-one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## training using gradually complete loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_epochs = 100\n",
    "counter = counter + 1\n",
    "profile = \"all-one\"\n",
    "# loss_mask = np.zeros(10)\n",
    "params['idp'] = np.arange(0.1,1.05,0.1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope(profile+str(counter)) as scope:\n",
    "        intv = 0\n",
    "        tidp = 0\n",
    "        var_reuse = False\n",
    "        go_on = True\n",
    "        current_best_accu = 0\n",
    "        current_best_counter = 0\n",
    "        while go_on:\n",
    "            # loss_mask[tidp] = 1\n",
    "            save_dir = \"~/IDP/output/\"\n",
    "            epochs = 1\n",
    "            batch_per_epoch = 200\n",
    "            batch_size = 28\n",
    "            learning_rate = 0.001\n",
    "            config = {'epochs': epochs,\n",
    "                      'batch_per_epoch': batch_per_epoch,\n",
    "                      'batch_size': batch_size,\n",
    "                      'learning_rate': learning_rate,\n",
    "                      'save_dir': save_dir,\n",
    "                      'log_file': save_dir+profile+\"_\"+str(tidp*100)+\"_log.csv\",\n",
    "                      'result_file': save_dir+profile+\"_\"+str(tidp*100)+\"_result.csv\",\n",
    "                      'r2_file':save_dir+profile+\"_\"+str(tidp*100)+\"_r2.csv\",\n",
    "                      'mnist': mnist,\n",
    "                      'loss_counter':tidp\n",
    "                     }\n",
    "                      # 'loss_mask':loss_mask}\n",
    "    \n",
    "            \"\"\" result containers \"\"\"\n",
    "            profile_arr = []\n",
    "            idp_arr = []\n",
    "            accu_arr = []\n",
    "            r2_dict = {}\n",
    "            log = pd.DataFrame()\n",
    "            \n",
    "            print(\"============ ITERATIVE TRAINING ============\")\n",
    "            #for _ in range(int(all_epochs/epochs/2)):  \n",
    "            intv = intv + 1\n",
    "            print(\"Start \"+str(intv)+\" round\")\n",
    "\n",
    "            ''' training W,b '''\n",
    "            if intv == 1:\n",
    "                model_test = model(profile,params=params)\n",
    "                model_test.loss_counter = tidp\n",
    "                print(\"Start to optimize IDP = %d \" % (tidp+1)*10)\n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=True)\n",
    "                log = pd.DataFrame.from_dict(log1)\n",
    "                ''' record initial gamma of layer 2 '''\n",
    "                r2_dict['initial'] = sess.run(model_test.r2)\n",
    "                var_reuse = True\n",
    "            else:\n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=True)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "                log = pd.concat([log,log1])\n",
    "\n",
    "            ''' test after W,b updates '''\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                #print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                profile_arr.append(profile+\"(train W,b in the \"+str(intv)+\" epochs)\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "\n",
    "            ''' training gamma '''\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            log1 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=var_reuse,verbose=False)\n",
    "            log1 = pd.DataFrame.from_dict(log1)\n",
    "            log = pd.concat([log,log1])\n",
    "            r2_dict['after_'+str(intv)] = sess.run(model_test.r2)\n",
    "\n",
    "            ''' test after gamma updates '''\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            this_accu = []\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                # print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                profile_arr.append(profile+\"(train gamma in the \"+str(intv)+\" epochs)\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "                # accu list\n",
    "                this_accu.append(accu)\n",
    "            this_accu = np.sum(np.array(this_accu))\n",
    "            if this_accu > current_best_accu:\n",
    "                current_best_accu = this_accu\n",
    "                current_best_counter = 0\n",
    "            else:\n",
    "                current_best_counter = current_best_counter + 1\n",
    "                \n",
    "            # if converged and wait for 3 epochs, next idp point\n",
    "            if (current_best_counter >= 3) & (this_accu >= 19.0):\n",
    "                tidp = tidp + 1\n",
    "                current_best_counter = 0\n",
    "                model_test.loss_counter = tidp\n",
    "                print(\"Start to optimize IDP = %d \" % (tidp+1)*10)\n",
    "            \n",
    "            # go over the whole tidp point\n",
    "            if tidp == 10:\n",
    "                go_on = False\n",
    "            \n",
    "                                      \n",
    "            r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "            r2.to_csv(config['r2_file'],index=None)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            res.to_csv(config['result_file'],index=None)\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alternate training using different loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_idp = [0.05]\n",
    "all_epochs = 20\n",
    "counter = counter + 1\n",
    "profile = \"all-one\"\n",
    "for tidp in target_idp:\n",
    "    params['idp'] = np.array([tidp])\n",
    "    interval = 1\n",
    "    save_dir = \"~/IDP/output/\"\n",
    "    epochs = interval\n",
    "    batch_per_epoch = 200\n",
    "    batch_size = 28\n",
    "    learning_rate = 0.001\n",
    "    config = {'epochs': epochs,\n",
    "              'batch_per_epoch': batch_per_epoch,\n",
    "              'batch_size': batch_size,\n",
    "              'learning_rate': learning_rate,\n",
    "              'save_dir': save_dir,\n",
    "              'log_file': save_dir+profile+\"_\"+str(tidp*100)+\"_log.csv\",\n",
    "              'result_file': save_dir+profile+\"_\"+str(tidp*100)+\"_result.csv\",\n",
    "              'r2_file':save_dir+profile+\"_\"+str(tidp*100)+\"_r2.csv\",\n",
    "              'mnist': mnist}\n",
    "    print(profile)\n",
    "    \n",
    "    '''result containers'''\n",
    "    profile_arr = []\n",
    "    idp_arr = []\n",
    "    accu_arr = []\n",
    "    r2_dict = {}\n",
    "    log = pd.DataFrame()\n",
    "    with tf.Session() as sess:\n",
    "        with tf.variable_scope(str(tidp)+str(counter)+str(epochs)) as scope:            \n",
    "            print(\"============ ITERATIVE TRAINING ============\")\n",
    "            var_reuse = False\n",
    "            for intv in range(int(all_epochs/interval/2)):  \n",
    "                print(\"Start \"+str(intv)+\" round\")\n",
    "                '''training W,b'''\n",
    "                if intv == 0:\n",
    "                    model_test = model(profile,params=params)\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log = pd.DataFrame.from_dict(log1)\n",
    "                    ''' record initial gamma of layer 2'''\n",
    "                    r2_dict['initial'] = sess.run(model_test.r2)\n",
    "                    var_reuse = True\n",
    "                else:\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log1 = pd.DataFrame.from_dict(log1)\n",
    "                    log = pd.concat([log,log1])\n",
    "                \n",
    "                '''test after W,b updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train W,b in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                \n",
    "                '''training gamma'''\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=var_reuse,verbose=False)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "                log = pd.concat([log,log1])\n",
    "                r2_dict['after_'+str(intv)] = sess.run(model_test.r2)\n",
    "                \n",
    "                '''test after gamma updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train gamma in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                                  \n",
    "            r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "            r2.to_csv(config['r2_file'],index=None)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            res.to_csv(config['result_file'],index=None)\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alternate training between gamma and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_epochs = 20\n",
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    save_dir = \"~/IDP/output/\"\n",
    "    epochs = 1\n",
    "    batch_per_epoch = 200\n",
    "    batch_size = 28\n",
    "    learning_rate = 0.001\n",
    "    config = {'epochs': epochs,\n",
    "              'batch_per_epoch': batch_per_epoch,\n",
    "              'batch_size': batch_size,\n",
    "              'learning_rate': learning_rate,\n",
    "              'save_dir': save_dir,\n",
    "              'log_file': save_dir+profile+\"_\"+str(epochs)+\"_log.csv\",\n",
    "              'result_file': save_dir+profile+\"_\"+str(epochs)+\"_result.csv\",\n",
    "              'r2_file':save_dir+profile+\"_\"+str(epochs)+\"_r2.csv\",\n",
    "              'mnist': mnist}\n",
    "    print(profile)\n",
    "    \n",
    "    '''result containers'''\n",
    "    profile_arr = []\n",
    "    idp_arr = []\n",
    "    accu_arr = []\n",
    "    r2_dict = {}\n",
    "    log = pd.DataFrame()\n",
    "    with tf.Session() as sess:\n",
    "        with tf.variable_scope(profile+str(counter)+str(epochs)) as scope:            \n",
    "            print(\"============ ITERATIVE TRAINING ============\")\n",
    "            var_reuse = False\n",
    "            for intv in range(int(all_epochs/epoch/2)):  \n",
    "                print(\"Start \"+str(intv)+\" round\")\n",
    "                '''training W,b'''\n",
    "                if intv == 0:\n",
    "                    model_test = model(profile,params=params)\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log = pd.DataFrame.from_dict(log1)\n",
    "                    ''' record initial gamma of layer 2'''\n",
    "                    r2_dict['initial'] = sess.run(model_test.r2)\n",
    "                    var_reuse = True\n",
    "                else:\n",
    "                    log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=var_reuse,verbose=False)\n",
    "                    log1 = pd.DataFrame.from_dict(log1)\n",
    "                    log = pd.concat([log,log1])\n",
    "                \n",
    "                '''test after W,b updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train W,b in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                \n",
    "                '''training gamma'''\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=var_reuse,verbose=False)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "                log = pd.concat([log,log1])\n",
    "                r2_dict['after_'+str(intv)] = sess.run(model_test.r2)\n",
    "                \n",
    "                '''test after gamma updates'''\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    print(\"IDP={:.2f}, accuracy={:.3f}\".format(idp,accu))\n",
    "                    profile_arr.append(profile+\"(train gamma in the \"+str(intv)+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                                  \n",
    "            r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "            r2.to_csv(config['r2_file'],index=None)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            res.to_csv(config['result_file'],index=None)\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Study on when we start training gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs_test = np.array([5])\n",
    "profile_test = [\"linear\",\"half-exp\",\"all-one\",\"harmonic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_epochs = 100\n",
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    for epochs in epochs_test:\n",
    "        save_dir = \"~/IDP/output/\"\n",
    "        # epochs = 20\n",
    "        batch_per_epoch = 200\n",
    "        batch_size = 28\n",
    "        learning_rate = 0.001\n",
    "        config = {'epochs': epochs,\n",
    "                  'batch_per_epoch': batch_per_epoch,\n",
    "                  'batch_size': batch_size,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'save_dir': save_dir,\n",
    "                  'log_file': save_dir+profile+\"_\"+str(epochs)+\"_log.csv\",\n",
    "                  'result_file': save_dir+profile+\"_\"+str(epochs)+\"_result.csv\",\n",
    "                  'r2_file':save_dir+profile+\"_\"+str(epochs)+\"_r2.csv\",\n",
    "                  'mnist': mnist}\n",
    "        print(profile)\n",
    "        '''result containers'''\n",
    "        profile_arr = []\n",
    "        idp_arr = []\n",
    "        accu_arr = []\n",
    "        r2_dict = {}\n",
    "        with tf.Session() as sess:\n",
    "            with tf.variable_scope(profile+str(counter)+str(epochs)) as scope:\n",
    "                model_test = model(profile,params=params)\n",
    "                log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "                log1 = pd.DataFrame.from_dict(log1)\n",
    "\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+str(epochs)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    profile_arr.append(profile+\"(train W,b in the first \"+str(config['epochs'])+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                    \n",
    "                ''' record current gamma of layer 2'''\n",
    "                r2_dict['before'] = sess.run(model_test.r2)\n",
    "                \n",
    "                print(\"============ FURTHER TRAINING ============\")\n",
    "                scope.reuse_variables()\n",
    "                config['epochs'] = all_epochs - config['epochs']\n",
    "                log2 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=True,verbose=False)\n",
    "                log2 = pd.DataFrame.from_dict(log2)\n",
    "                log = pd.concat([log1,log2])\n",
    "                log.to_csv(config['log_file'],index=None)\n",
    "                #print(log)\n",
    "\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope=profile+str(counter)+str(epochs)+'i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    profile_arr.append(profile+\"(train gamma in the last \"+str(config['epochs'])+\" epochs)\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                    \n",
    "                ''' record current gamma of layer 2'''\n",
    "                r2_dict['after'] = sess.run(model_test.r2)\n",
    "                \n",
    "                r2 = pd.DataFrame.from_dict(r2_dict)\n",
    "                r2.to_csv(config['r2_file'],index=None)\n",
    "                \n",
    "                res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "                res.to_csv(config['result_file'],index=None)\n",
    "                print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Half-half training experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    save_dir = \"~/IDP/output/\"\n",
    "    epochs = 1\n",
    "    batch_per_epoch = 200\n",
    "    batch_size = 28\n",
    "    learning_rate = 0.001\n",
    "    config = {'epochs': epochs,\n",
    "              'batch_per_epoch': batch_per_epoch,\n",
    "              'batch_size': batch_size,\n",
    "              'learning_rate': learning_rate,\n",
    "              'save_dir': save_dir,\n",
    "              'log_file': save_dir+profile+\"_log.csv\",\n",
    "              'result_file': save_dir+profile+\"_result.csv\",\n",
    "              'mnist': mnist}\n",
    "    print(profile)\n",
    "    '''result containers'''\n",
    "    profile_arr = []\n",
    "    idp_arr = []\n",
    "    accu_arr = []\n",
    "    with tf.Session() as sess:\n",
    "        with tf.variable_scope(profile+str(counter)) as scope:\n",
    "            model_test = model(profile,params=params)\n",
    "            log1 = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "            log1 = pd.DataFrame.from_dict(log1)\n",
    "\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                profile_arr.append(profile+\"(fixed gamma ep\"+str(config['epochs'])+\")\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "\n",
    "            print(\"============ FURTHER TRAINING ============\")\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            log2 = model_test.train(sess=sess,config=config,gamma_trainable=True,reuse=True,verbose=False)\n",
    "            log2 = pd.DataFrame.from_dict(log2)\n",
    "            log = pd.concat([log1,log2])\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            #print(log)\n",
    "\n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                profile_arr.append(profile+\"(trainable gamma ep\"+str(config['epochs'])+\")\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "            #res = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            #res.to_csv(config['result_file'],index=None)\n",
    "            #print(res)\n",
    "            \n",
    "        print(\"============ TRAINING AS USUAL ============\")\n",
    "        \n",
    "        with tf.variable_scope(profile+str(counter)+\"normal\"):\n",
    "            model_test = model(profile=profile,params=params)\n",
    "            save_dir = \"~/IDP/output/\"\n",
    "            epochs = 80\n",
    "            batch_per_epoch = 200\n",
    "            batch_size = 28\n",
    "            learning_rate = 0.001\n",
    "            config = {'epochs': epochs,\n",
    "                      'batch_per_epoch': batch_per_epoch,\n",
    "                      'batch_size': batch_size,\n",
    "                      'learning_rate': learning_rate,\n",
    "                      'save_dir': save_dir,\n",
    "                      'log_file': save_dir+profile+\"_log2.csv\",\n",
    "                      'result_file': save_dir+profile+\"_result.csv\",\n",
    "                      'mnist': mnist}\n",
    "            with tf.Session() as sess:\n",
    "                log = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "                log = pd.DataFrame.from_dict(log)\n",
    "                log.to_csv(config['log_file'],index=None)\n",
    "                true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "                for idp in np.arange(0.1,1.05,0.05):  \n",
    "                    probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                    pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                    accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                    profile_arr.append(profile+\"(fixed gamma ep\"+str(config['epochs'])+\")\")\n",
    "                    idp_arr.append(idp)\n",
    "                    accu_arr.append(accu)\n",
    "                log = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "                log.to_csv(config['result_file'],index=None)\n",
    "                print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter = counter + 1\n",
    "for profile in profile_test:\n",
    "    print(profile)\n",
    "    with tf.variable_scope(profile+str(counter)):\n",
    "        model_test = model(profile=profile,params=params)\n",
    "        save_dir = \"~/IDP/output/new_loss_\"\n",
    "        epochs = 20\n",
    "        batch_per_epoch = 200\n",
    "        batch_size = 28\n",
    "        learning_rate = 0.002\n",
    "        config = {'epochs': epochs,\n",
    "                  'batch_per_epoch': batch_per_epoch,\n",
    "                  'batch_size': batch_size,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'save_dir': save_dir,\n",
    "                  'log_file': save_dir+profile+\"_log.csv\",\n",
    "                  'result_file': save_dir+profile+\"_result.csv\",\n",
    "                  'mnist': mnist}\n",
    "        with tf.Session() as sess:\n",
    "            log = model_test.train(sess=sess,config=config,gamma_trainable=False,reuse=False,verbose=False)\n",
    "            log = pd.DataFrame.from_dict(log)\n",
    "            log.to_csv(config['log_file'],index=None)\n",
    "            print(log)\n",
    "            \n",
    "            true_lab = [np.argmax(ite) for ite in mnist.test.labels]\n",
    "            profile_arr = []\n",
    "            idp_arr = []\n",
    "            accu_arr = []\n",
    "            for idp in np.arange(0.1,1.05,0.05):  \n",
    "                probs = model_test.predict(sess=sess,config=config,idp=idp,scope='i'+str(int(idp*100)))\n",
    "                pred_lab = [np.argmax(ite) for ite in probs[0]]\n",
    "                accu = accuracy_score(y_pred=pred_lab,y_true=true_lab)\n",
    "                profile_arr.append(profile+\"(fixed gamma ep\"+str(config['epochs'])+\")\")\n",
    "                idp_arr.append(idp)\n",
    "                accu_arr.append(accu)\n",
    "            log = pd.DataFrame.from_dict({'profile':profile_arr,'IDP':idp_arr,'accu':accu_arr})\n",
    "            log.to_csv(config['result_file'],index=None)\n",
    "            print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
